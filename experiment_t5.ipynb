{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Flan-T5-Base with LoRA for Dialogue Summarization\n",
    "\n",
    "This notebook walks through fine-tuning `google/flan-t5-base` using **LoRA** (Low-Rank Adaptation) on the `knkarthick/dialogsum` dataset.\n",
    "\n",
    "Key features:\n",
    "- 4-bit quantization via `BitsAndBytesConfig` to save VRAM\n",
    "- LoRA with r=16, alpha=32 targeting the q and v projection modules\n",
    "- Training on the first 500 rows of the dataset\n",
    "- Saving and reloading only the LoRA adapters for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell if the packages are not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 2)) (5.1.0)\n",
      "Requirement already satisfied: peft in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 3)) (0.18.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 4)) (4.5.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 5)) (0.49.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 6)) (1.12.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch->-r requirements.txt (line 1)) (75.8.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from peft->-r requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->-r requirements.txt (line 4)) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets->-r requirements.txt (line 4)) (0.70.18)\n",
      "Requirement already satisfied: absl-py in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge-score->-r requirements.txt (line 8)) (2.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge-score->-r requirements.txt (line 8)) (3.9.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge-score->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (3.11.12)\n",
      "Requirement already satisfied: anyio in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 4)) (1.26.20)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.27->transformers->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: click in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk->rouge-score->-r requirements.txt (line 8)) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk->rouge-score->-r requirements.txt (line 8)) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2025.3)\n",
      "Requirement already satisfied: typer>=0.23.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from typer-slim->transformers->-r requirements.txt (line 2)) (0.23.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.18.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from typer>=0.23.0->typer-slim->transformers->-r requirements.txt (line 2)) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from typer>=0.23.0->typer-slim->transformers->-r requirements.txt (line 2)) (0.0.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio->httpx<1.0.0->datasets->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers->-r requirements.txt (line 2)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mobol\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->transformers->-r requirements.txt (line 2)) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\mobol\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below to install from requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobol\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Model and dataset --\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "DATASET_NAME = \"knkarthick/dialogsum\"\n",
    "NUM_TRAIN_SAMPLES = 500\n",
    "\n",
    "# -- Paths --\n",
    "OUTPUT_DIR = \"./lora-flan-t5-summarization\"\n",
    "ADAPTER_DIR = \"./lora-adapters\"\n",
    "\n",
    "# -- Tokenization --\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "# -- Training hyperparameters --\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 3\n",
    "LOGGING_STEPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer and Quantized Model\n",
    "\n",
    "We use `BitsAndBytesConfig` with 4-bit NF4 quantization and double quantization enabled to minimize memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BitsAndBytesConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4-bit quantization config\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m bnb_config = \u001b[43mBitsAndBytesConfig\u001b[49m(\n\u001b[32m      3\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      4\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     bnb_4bit_compute_dtype=torch.bfloat16,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[32m     10\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
      "\u001b[31mNameError\u001b[39m: name 'BitsAndBytesConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training (freeze base weights, cast layernorm to fp32, etc.)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA Adapters\n",
    "\n",
    "LoRA configuration:\n",
    "- **r = 16** (rank of the low-rank matrices)\n",
    "- **alpha = 32** (scaling factor)\n",
    "- **target_modules = [\"q\", \"v\"]** (query and value projection layers)\n",
    "- **task_type = SEQ_2_SEQ_LM** (sequence-to-sequence language modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 249,347,328 || trainable%: 0.7096\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Preprocess the Dataset\n",
    "\n",
    "We use the first 500 rows from `knkarthick/dialogsum` for training and up to 100 rows for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 500\n",
      "Validation samples: 100\n",
      "\n",
      "Sample dialogue (truncated):\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doc...\n",
      "\n",
      "Sample summary:\n",
      "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "train_dataset = dataset[\"train\"].select(range(NUM_TRAIN_SAMPLES))\n",
    "val_dataset = dataset[\"validation\"].select(range(min(100, len(dataset[\"validation\"]))))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"\\nSample dialogue (truncated):\\n{train_dataset[0]['dialogue'][:300]}...\")\n",
    "print(f\"\\nSample summary:\\n{train_dataset[0]['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training features: ['input_ids', 'attention_mask', 'labels']\n",
      "Tokenized validation features: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "PREFIX = \"Summarize the following dialogue:\\n\\n\"\n",
    "\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [PREFIX + dialogue for dialogue in examples[\"dialogue\"]]\n",
    "    targets = examples[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Replace pad token ids in labels with -100 so they are ignored by the loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_val = val_dataset.map(\n",
    "    preprocess, batched=True, remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized training features: {tokenized_train.column_names}\")\n",
    "print(f\"Tokenized validation features: {tokenized_val.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up Evaluation Metrics (ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Decode predictions\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Decode labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Strip whitespace\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your setup doesn't support bf16/gpu. You need to assign use_cpu if you want to train the model on CPU",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m training_args = \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOGGING_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_max_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_TARGET_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrougeL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\u001b[32m     20\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     21\u001b[39m     model=model,\n\u001b[32m     22\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     28\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:117\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, enable_jit_checkpoint, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, use_cpu, seed, data_seed, bf16, fp16, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_config, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_for_metrics, eval_do_concat_batches, auto_find_batch_size, full_determinism, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, use_cache, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1534\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1532\u001b[39m                 error_message += \u001b[33m\"\u001b[39m\u001b[33m You need Ampere+ GPU with cuda>=11.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1533\u001b[39m             \u001b[38;5;66;03m# gpu\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1534\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16:\n\u001b[32m   1537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt most one of fp16 and bf16 can be True, but not both\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Your setup doesn't support bf16/gpu. You need to assign use_cpu if you want to train the model on CPU"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save LoRA Adapters\n",
    "\n",
    "We save only the LoRA adapter weights - not the full base model. This keeps the saved checkpoint very small (a few MB instead of hundreds of MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the LoRA adapters and tokenizer\n",
    "model.save_pretrained(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_DIR)\n",
    "\n",
    "print(f\"LoRA adapters saved to: {ADAPTER_DIR}\")\n",
    "\n",
    "# Show what was saved\n",
    "adapter_files = os.listdir(ADAPTER_DIR)\n",
    "print(f\"\\nSaved files:\")\n",
    "for f in adapter_files:\n",
    "    size = os.path.getsize(os.path.join(ADAPTER_DIR, f))\n",
    "    print(f\"  {f} ({size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Reload Adapters and Run Inference\n",
    "\n",
    "This section demonstrates how to load the saved LoRA adapters onto a fresh base model and run inference on a custom input string. This is the pattern you would use in production or on a different machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Reload from scratch to prove it works independently --\n",
    "\n",
    "# Step 1: Load the base model again with 4-bit quantization\n",
    "reload_bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=reload_bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Step 2: Load the LoRA adapters on top of the base model\n",
    "inference_model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)\n",
    "\n",
    "print(\"Base model + LoRA adapters loaded successfully for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Run inference on a custom dialogue string --\n",
    "\n",
    "custom_dialogue = (\n",
    "    \"Sarah: Hey Mike, are we still on for the project meeting tomorrow?\\n\"\n",
    "    \"Mike: Yes, but can we push it to 2 PM instead of 10 AM? I have a dentist appointment in the morning.\\n\"\n",
    "    \"Sarah: Sure, 2 PM works. Should I book the conference room?\\n\"\n",
    "    \"Mike: That would be great. Also, can you send me the latest draft before the meeting?\\n\"\n",
    "    \"Sarah: Will do. I will email it to you tonight.\\n\"\n",
    "    \"Mike: Perfect. Thanks, Sarah!\"\n",
    ")\n",
    "\n",
    "input_text = PREFIX + custom_dialogue\n",
    "\n",
    "inputs = inference_tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_INPUT_LENGTH,\n",
    "    truncation=True,\n",
    ").to(inference_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = inference_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_TARGET_LENGTH,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "summary = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIALOGUE:\")\n",
    "print(\"=\" * 60)\n",
    "print(custom_dialogue)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATED SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Helper function for quick inference on any dialogue --\n",
    "\n",
    "def summarize_dialogue(dialogue: str, model=inference_model, tok=inference_tokenizer) -> str:\n",
    "    \"\"\"Summarize a dialogue string using the fine-tuned LoRA model.\"\"\"\n",
    "    prompt = PREFIX + dialogue\n",
    "    inputs = tok(prompt, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=MAX_TARGET_LENGTH, num_beams=4, early_stopping=True)\n",
    "    return tok.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Try it out with another example\n",
    "another_dialogue = (\n",
    "    \"Tom: Did you see the email from the client?\\n\"\n",
    "    \"Jane: Yes, they want the delivery moved up by two weeks.\\n\"\n",
    "    \"Tom: That is going to be tight. Can we pull it off?\\n\"\n",
    "    \"Jane: If we get two more developers, maybe. I will talk to the manager.\\n\"\n",
    "    \"Tom: Alright, keep me posted.\"\n",
    ")\n",
    "\n",
    "print(\"Summary:\", summarize_dialogue(another_dialogue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "You have successfully:\n",
    "1. Loaded `google/flan-t5-base` with 4-bit quantization\n",
    "2. Applied LoRA adapters (r=16, alpha=32) to the q and v modules\n",
    "3. Trained on 500 samples from `knkarthick/dialogsum`\n",
    "4. Saved the lightweight LoRA adapters\n",
    "5. Reloaded them onto a fresh base model for inference\n",
    "\n",
    "To use these adapters elsewhere, just copy the `lora-adapters/` directory and follow the reload pattern in section 11."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
